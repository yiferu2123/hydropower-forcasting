{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":14592283,"sourceType":"datasetVersion","datasetId":9321055}],"dockerImageVersionId":31260,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Environment Setup & Library Imports\nThis cell installs the necessary `dash` library (if not already present) and imports core packages for data manipulation (Pandas, NumPy), visualization (Plotly, Dash), and machine learning (TensorFlow, Scikit-Learn, XGBoost). It also suppresses warnings to ensure a clean output.","metadata":{}},{"cell_type":"code","source":"pip install jupyter_dash","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T07:28:45.740694Z","iopub.execute_input":"2026-01-27T07:28:45.741265Z","iopub.status.idle":"2026-01-27T07:28:49.348639Z","shell.execute_reply.started":"2026-01-27T07:28:45.741228Z","shell.execute_reply":"2026-01-27T07:28:49.347520Z"}},"outputs":[{"name":"stdout","text":"Collecting jupyter_dash\n  Downloading jupyter_dash-0.4.2-py3-none-any.whl.metadata (3.6 kB)\nRequirement already satisfied: dash in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (3.4.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (2.32.5)\nRequirement already satisfied: flask in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (3.1.2)\nRequirement already satisfied: retrying in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (1.4.2)\nRequirement already satisfied: ipython in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (7.34.0)\nRequirement already satisfied: ipykernel in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (6.17.1)\nCollecting ansi2html (from jupyter_dash)\n  Downloading ansi2html-1.9.2-py3-none-any.whl.metadata (3.7 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.12/dist-packages (from jupyter_dash) (1.6.0)\nRequirement already satisfied: Werkzeug<3.2 in /usr/local/lib/python3.12/dist-packages (from dash->jupyter_dash) (3.1.3)\nRequirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.12/dist-packages (from dash->jupyter_dash) (5.24.1)\nRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.12/dist-packages (from dash->jupyter_dash) (8.7.0)\nRequirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.12/dist-packages (from dash->jupyter_dash) (4.15.0)\nRequirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from dash->jupyter_dash) (75.2.0)\nRequirement already satisfied: blinker>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from flask->jupyter_dash) (1.9.0)\nRequirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.12/dist-packages (from flask->jupyter_dash) (8.3.1)\nRequirement already satisfied: itsdangerous>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from flask->jupyter_dash) (2.2.0)\nRequirement already satisfied: jinja2>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from flask->jupyter_dash) (3.1.6)\nRequirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from flask->jupyter_dash) (3.0.3)\nRequirement already satisfied: debugpy>=1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (1.8.15)\nRequirement already satisfied: jupyter-client>=6.1.12 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (7.4.9)\nRequirement already satisfied: matplotlib-inline>=0.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (0.1.7)\nRequirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (26.0rc2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (5.9.5)\nRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (26.2.1)\nRequirement already satisfied: tornado>=6.1 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (6.5.1)\nRequirement already satisfied: traitlets>=5.1.0 in /usr/local/lib/python3.12/dist-packages (from ipykernel->jupyter_dash) (5.7.1)\nRequirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (0.19.2)\nRequirement already satisfied: decorator in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (4.4.2)\nRequirement already satisfied: pickleshare in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (0.7.5)\nRequirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (3.0.52)\nRequirement already satisfied: pygments in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (2.19.2)\nRequirement already satisfied: backcall in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (0.2.0)\nRequirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.12/dist-packages (from ipython->jupyter_dash) (4.9.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->jupyter_dash) (3.4.4)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->jupyter_dash) (3.11)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->jupyter_dash) (2.6.3)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->jupyter_dash) (2026.1.4)\nRequirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from jedi>=0.16->ipython->jupyter_dash) (0.8.5)\nRequirement already satisfied: entrypoints in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter_dash) (0.4)\nRequirement already satisfied: jupyter-core>=4.9.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter_dash) (5.9.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from jupyter-client>=6.1.12->ipykernel->jupyter_dash) (2.9.0.post0)\nRequirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.12/dist-packages (from pexpect>4.3->ipython->jupyter_dash) (0.7.0)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly>=5.0.0->dash->jupyter_dash) (9.1.2)\nRequirement already satisfied: wcwidth in /usr/local/lib/python3.12/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython->jupyter_dash) (0.2.14)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata->dash->jupyter_dash) (3.23.0)\nRequirement already satisfied: platformdirs>=2.5 in /usr/local/lib/python3.12/dist-packages (from jupyter-core>=4.9.2->jupyter-client>=6.1.12->ipykernel->jupyter_dash) (4.5.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->jupyter-client>=6.1.12->ipykernel->jupyter_dash) (1.17.0)\nDownloading jupyter_dash-0.4.2-py3-none-any.whl (23 kB)\nDownloading ansi2html-1.9.2-py3-none-any.whl (17 kB)\nInstalling collected packages: ansi2html, jupyter_dash\nSuccessfully installed ansi2html-1.9.2 jupyter_dash-0.4.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"\n# Install necessary packages if running in a new environment\n# !pip install dash pandas numpy scikit-learn tensorflow xgboost plotly\nimport os\nimport io\nimport base64\nimport datetime\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nimport plotly.express as px\nfrom dash import Dash, dcc, html, Input, Output, State, callback_context\n# from jupyter_dash import JupyterDash # Deprecated, using standard Dash with jupyter_mode\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport xgboost as xgb\nimport warnings\n\nimport ipywidgets as widgets\nfrom IPython.display import display\nimport plotly.graph_objects as go\nwarnings.filterwarnings('ignore')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T07:50:39.463493Z","iopub.execute_input":"2026-01-27T07:50:39.464448Z","iopub.status.idle":"2026-01-27T07:50:39.469714Z","shell.execute_reply.started":"2026-01-27T07:50:39.464414Z","shell.execute_reply":"2026-01-27T07:50:39.469180Z"}},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":"# 2. Data Configuration & Helper Functions\n**# ** Here we define the dataset file paths and categorize columns (Dates, Weather, Power). We also implement helper functions to identify column types and standardise numeric values, ensuring consistent data formatting across different datasets.**","metadata":{}},{"cell_type":"code","source":"# Adapting paths for local environment or Kaggle\nDATA_DIR = \"/kaggle/input/dataset1\"\ndatasets = {\n    \"Gibe I\": \"Gibe1.csv\",\n    \"Gibe III\": \"Gibe3.csv\",\n    \"Koka\": \"Koka Plant.csv\",\n    \"Tana Beles\": \"Tana_Beles.csv\",\n    \"Tekeze\": \"Tekeze.csv\",\n    \"Fincha\": \"fincha.csv\",\n}\nDATE_COLS = ['Date_GC', 'Date_EC']\nWEATHER_COLS = ['T2M', 'PRECTOTCORR', 'ALLSKY_SFC_SW_DWN', 'RH2M', 'WS2M']\nPOWER_PREFIXES = ['U', 'V', 'W', 'Max_ALoad', 'Min_ALoad', 'Auxiliary', 'Water_Level', 'Energy', 'Discharge']\ndef identify_columns(df):\n    cols = df.columns.tolist()\n    date_c = [c for c in cols if c in DATE_COLS]\n    weather_c = [c for c in cols if c in WEATHER_COLS]\n    power_c = [c for c in cols if any(c.startswith(p) for p in POWER_PREFIXES) and c not in weather_c]\n    other_c = [c for c in cols if c not in date_c + weather_c + power_c and c not in ['Date_EC', 'Date']]\n    return date_c, weather_c, power_c, other_c\ndef clean_numeric(x):\n    if isinstance(x, str):\n        x = x.replace(',', '').replace(' ', '')\n    return pd.to_numeric(x, errors='coerce')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T07:50:45.446297Z","iopub.execute_input":"2026-01-27T07:50:45.446585Z","iopub.status.idle":"2026-01-27T07:50:45.453178Z","shell.execute_reply.started":"2026-01-27T07:50:45.446561Z","shell.execute_reply":"2026-01-27T07:50:45.452366Z"}},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":"# 3. Data Loading & Logic Separation\n*This section separates the preprocessing into more granular steps. `load_raw_formatted` handles loading and initial formatting, while `apply_cleaning` applies imputation and outlier treatments. This modularity enables the \"Before vs. After\" comparison. We then batch-process all datasets into `data_store` for modeling.","metadata":{}},{"cell_type":"code","source":"# %% [code]\ndef load_raw_formatted(filepath):\n    try:\n        df = pd.read_csv(filepath)\n    except FileNotFoundError:\n        return None\n    \n    date_c, weather_c, power_c, other_c = identify_columns(df)\n    \n    # 1. Date Formatting\n    if 'Date_GC' in df.columns:\n        df['Date'] = pd.to_datetime(df['Date_GC'])\n        df.set_index('Date', inplace=True)\n        df.sort_index(inplace=True)\n        df.drop(columns=[c for c in DATE_COLS if c in df.columns], inplace=True)\n    \n    # 2. Numeric Formatting (Dirty Phase)\n    all_numeric = weather_c + power_c + other_c\n    for col in all_numeric:\n        if col in df.columns:\n            df[col] = df[col].apply(clean_numeric)\n            \n    return df\ndef apply_cleaning(df_in):\n    df = df_in.copy()\n    date_c, weather_c, power_c, other_c = identify_columns(df)\n    \n    # 1. Weather Columns: Fill Mean + Clip IQR\n    for col in weather_c:\n        if col in df.columns:\n            df[col] = df[col].fillna(df[col].mean())\n            Q1 = df[col].quantile(0.25)\n            Q3 = df[col].quantile(0.75)\n            IQR = Q3 - Q1\n            df[col] = np.clip(df[col], Q1 - 1.5 * IQR, Q3 + 1.5 * IQR)\n    # 2. Power/Other Columns: Fill 0 + Clip 5th-95th Percentile\n    for col in power_c + other_c:\n        if col in df.columns:\n            # For visualization purpose, we might want to catch NaNs here mostly\n            df[col] = df[col].fillna(0)\n            # Clip\n            low = df[col].quantile(0.05)\n            high = df[col].quantile(0.95)\n            df[col] = np.clip(df[col], low, high)\n            \n    return df\n# Load Processed Data for Modeling\ndata_store = {}\nprint(\"Loading data for modeling...\")\nfor name, file in datasets.items():\n    path = os.path.join(DATA_DIR, file)\n    if os.path.exists(path):\n        raw = load_raw_formatted(path)\n        if raw is not None:\n            cleaned = apply_cleaning(raw)\n            data_store[name] = cleaned\n            print(f\"Loaded & Cleaned {name}: {cleaned.shape}\")\n    else:\n        print(f\"Warning: {file} NOT FOUND at {path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T07:50:49.484573Z","iopub.execute_input":"2026-01-27T07:50:49.485051Z","iopub.status.idle":"2026-01-27T07:50:51.592033Z","shell.execute_reply.started":"2026-01-27T07:50:49.485026Z","shell.execute_reply":"2026-01-27T07:50:51.591320Z"}},"outputs":[{"name":"stdout","text":"Loading data for modeling...\nLoaded & Cleaned Gibe I: (4383, 13)\nLoaded & Cleaned Gibe III: (3653, 21)\nLoaded & Cleaned Koka: (4383, 14)\nLoaded & Cleaned Tana Beles: (4383, 15)\nLoaded & Cleaned Tekeze: (4383, 15)\nLoaded & Cleaned Fincha: (4383, 15)\n","output_type":"stream"}],"execution_count":18},{"cell_type":"markdown","source":"# # 4. Interactive Preprocessing Dashboard (Dash)\n This cell defines a Dash application to visualize the preprocessing steps.\n **Instructions:**\n 1. Run this cell.\n 2. Select a Plant Dataset from the dropdown.\n 3. Click \"Run Preprocessing Step\" to execute the cleaning logic on the raw data.\n 4. Observe the \"Before\" vs \"After\" metrics for Missing Values, Data Distribution (Box Plots), and general statistics.\n5. Note: This app runs inside the notebook output area.","metadata":{}},{"cell_type":"code","source":"# -----------------------------\n# Widgets\n# -----------------------------\nplant_dropdown = widgets.Dropdown(\n    options=list(datasets.keys()),\n    description='Plant:',\n    style={'description_width': 'initial'}\n)\n\nrun_button = widgets.Button(\n    description='Run Preprocessing Validation',\n    button_style='success',\n    icon='play'\n)\n\noutput = widgets.Output()\n\n# -----------------------------\n# Dashboard Logic\n# -----------------------------\ndef run_dashboard(b):\n    output.clear_output()\n    \n    with output:\n        plant = plant_dropdown.value\n        filepath = os.path.join(DATA_DIR, datasets[plant])\n        \n        # Load data (RAW & CLEANED)\n        df_before = load_raw_formatted(filepath)\n        df_after = apply_cleaning(df_before)\n        \n        date_c, weather_c, power_c, other_c = identify_columns(df_before)\n        \n        # =============================\n        # 1. Missing Values (Before vs After)\n        # =============================\n        miss_before = df_before.isnull().sum()\n        miss_after = df_after.isnull().sum()\n        \n        miss_cols = miss_before[miss_before > 0].index.tolist()\n        \n        fig_missing = go.Figure()\n        if miss_cols:\n            fig_missing.add_bar(\n                x=miss_cols,\n                y=miss_before[miss_cols],\n                name='Before'\n            )\n            fig_missing.add_bar(\n                x=miss_cols,\n                y=miss_after[miss_cols],\n                name='After'\n            )\n            fig_missing.update_layout(\n                title=f\"Missing Values Before vs After ‚Äì {plant}\",\n                barmode='group'\n            )\n        else:\n            fig_missing.update_layout(\n                title=f\"No Missing Values Detected ‚Äì {plant}\"\n            )\n        \n        # =============================\n        # 2. WEATHER OUTLIERS (IQR)\n        # =============================\n        fig_weather = go.Figure()\n        for col in weather_c:\n            if col in df_before.columns:\n                fig_weather.add_box(\n                    y=df_before[col],\n                    name=f\"{col} (Before)\",\n                    boxpoints='outliers'\n                )\n                fig_weather.add_box(\n                    y=df_after[col],\n                    name=f\"{col} (After)\",\n                    boxpoints='outliers'\n                )\n        \n        fig_weather.update_layout(\n            title=f\"Weather Columns ‚Äì IQR Outlier Treatment (Before vs After) ‚Äì {plant}\",\n            boxmode='group'\n        )\n        \n        # =============================\n        # 3. POWER / OTHER OUTLIERS (5‚Äì95% Capping)\n        # =============================\n        fig_power = go.Figure()\n        for col in (power_c + other_c):\n            if col in df_before.columns:\n                fig_power.add_box(\n                    y=df_before[col],\n                    name=f\"{col} (Before)\",\n                    boxpoints='outliers'\n                )\n                fig_power.add_box(\n                    y=df_after[col],\n                    name=f\"{col} (After)\",\n                    boxpoints='outliers'\n                )\n        \n        fig_power.update_layout(\n            title=f\"Power & Other Columns ‚Äì Percentile Capping (Before vs After) ‚Äì {plant}\",\n            boxmode='group'\n        )\n        \n        # =============================\n        # 4. Non-Null Count per Column\n        # =============================\n        non_null_before = df_before.count()\n        non_null_after = df_after.count()\n        \n        fig_nonnull = go.Figure()\n        fig_nonnull.add_bar(\n            x=non_null_before.index,\n            y=non_null_before.values,\n            name='Before'\n        )\n        fig_nonnull.add_bar(\n            x=non_null_after.index,\n            y=non_null_after.values,\n            name='After'\n        )\n        \n        fig_nonnull.update_layout(\n            title=f\"Non-Null Count per Column ‚Äì {plant}\",\n            barmode='group'\n        )\n        \n        # =============================\n        # 5. Summary Table\n        # =============================\n        display(widgets.HTML(f\"\"\"\n        <h3>Preprocessing Summary ‚Äì {plant}</h3>\n        <table border=\"1\" style=\"border-collapse:collapse\">\n            <tr><th>Metric</th><th>Before</th><th>After</th></tr>\n            <tr><td>Rows</td><td>{len(df_before)}</td><td>{len(df_after)}</td></tr>\n            <tr><td>Columns</td><td>{df_before.shape[1]}</td><td>{df_after.shape[1]}</td></tr>\n            <tr><td>Total Missing Values</td>\n                <td>{df_before.isnull().sum().sum()}</td>\n                <td>{df_after.isnull().sum().sum()}</td>\n            </tr>\n            <tr><td>Weather Columns (IQR)</td><td colspan=\"2\">{', '.join(weather_c)}</td></tr>\n            <tr><td>Power Columns (5‚Äì95%)</td><td colspan=\"2\">{', '.join(power_c)}</td></tr>\n        </table>\n        \"\"\"))\n        \n        # =============================\n        # Display Plots\n        # =============================\n        fig_missing.show()\n        fig_weather.show()\n        fig_power.show()\n        fig_nonnull.show()\n\n# -----------------------------\n# Bind & Display\n# -----------------------------\nrun_button.on_click(run_dashboard)\n\ndisplay(widgets.VBox([\n    plant_dropdown,\n    run_button,\n    output\n]))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T07:50:57.935085Z","iopub.execute_input":"2026-01-27T07:50:57.935712Z","iopub.status.idle":"2026-01-27T07:50:57.955837Z","shell.execute_reply.started":"2026-01-27T07:50:57.935686Z","shell.execute_reply":"2026-01-27T07:50:57.955024Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(Dropdown(description='Plant:', options=('Gibe I', 'Gibe III', 'Koka', 'Tana Beles', 'Tekeze', '‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06f421e4faeb4885a4e5cd3c1077b948"}},"metadata":{}}],"execution_count":19},{"cell_type":"markdown","source":"# # 5. Model Architecture & Training Utilities\nDefines the deep learning models (LSTM, GRU) and machine learning regressors (XGBoost). It includes the `train_predict_evaluate` function which handles feature engineering (seasonality), scaling, sequence generation, model training, and performance evaluation","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport datetime\nimport warnings\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\nimport xgboost as xgb\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import LSTM, GRU, Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\n\nwarnings.filterwarnings('ignore')\n\n# ---------- HELPERS ----------\n\ndef prepare_features_direct(df, target_col, feature_cols=None):\n    \"\"\"\n    Prepares data in the 'XGBoost Style': \n    - Adds time features (Year, Month, Day)\n    - Keep specified feature columns (weather, etc)\n    - Target column\n    \"\"\"\n    df = df.copy()\n    if feature_cols is None:\n        # Default fallback if not provided: use all numeric except target\n        feature_cols = [c for c in df.select_dtypes(include=np.number).columns if c != target_col]\n    \n    # Ensure date index or column availability (assuming df index is datetime based on context)\n    # If index is not datetime, try to find a date column or convert index\n    if not isinstance(df.index, pd.DatetimeIndex):\n         # Try to find a date column\n         date_col = next((c for c in df.columns if 'date' in c.lower()), None)\n         if date_col:\n             df[date_col] = pd.to_datetime(df[date_col])\n             df = df.set_index(date_col)\n    \n    df['month'] = df.index.month\n    df['day'] = df.index.day\n    df['dayofyear'] = df.index.dayofyear\n    df['year'] = df.index.year\n    \n    # Select features X and target y\n    # X includes: Provided Features + Time Features\n    x_cols = feature_cols + ['month', 'day', 'dayofyear', 'year']\n    \n    # Handle missing values in X (important for LSTM)\n    df[x_cols] = df[x_cols].fillna(df[x_cols].median())\n    df = df.dropna(subset=[target_col])\n    \n    X = df[x_cols].values\n    y = df[target_col].values\n    \n    return X, y, x_cols, df\n\ndef create_future_dataframe(df_historical, feature_cols, forecast_years):\n    \"\"\"\n    Generates future X input data by estimating future weather features \n    using historical monthly medians (Seasonality Preservation).\n    \"\"\"\n    last_date = df_historical.index[-1]\n    start_date = last_date + datetime.timedelta(days=1)\n    # Calculate end date based on float years (approx)\n    days_to_predict = int(forecast_years * 365)\n    end_date = start_date + datetime.timedelta(days=days_to_predict)\n    \n    future_dates = pd.date_range(start=start_date, end=end_date, freq='D')\n    future_df = pd.DataFrame(index=future_dates)\n    \n    future_df['month'] = future_df.index.month\n    future_df['day'] = future_df.index.day\n    future_df['dayofyear'] = future_df.index.dayofyear\n    future_df['year'] = future_df.index.year\n    \n    # Fill weather features using historical monthly medians\n    df_historical['month'] = df_historical.index.month # Ensure month col exists\n    monthly_stats = df_historical.groupby('month')[feature_cols].median().to_dict('index')\n    \n    for col in feature_cols:\n        if col in df_historical.columns:\n            future_df[col] = future_df['month'].map(lambda x: monthly_stats.get(x, {}).get(col, 0))\n        else:\n            future_df[col] = 0\n            \n    # Assemble final X matrix\n    x_cols = feature_cols + ['month', 'day', 'dayofyear', 'year']\n    return future_df[x_cols].values, future_dates\n\ndef build_dl_regressor(model_type, input_dim):\n    \"\"\"\n    Builds an LSTM or GRU model configured as a direct regressor \n    (Input: [1, Features], Output: [1])\n    \"\"\"\n    model = Sequential()\n    # Reshape input to (Samples, 1, Features) implicitly via input_shape\n    if model_type == 'LSTM':\n        model.add(LSTM(64, input_shape=(1, input_dim), return_sequences=False))\n    elif model_type == 'GRU':\n        model.add(GRU(64, input_shape=(1, input_dim), return_sequences=False))\n        \n    model.add(Dropout(0.2))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(1)) # Linear output for regression\n    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n    return model\n\n# ---------- MAIN FUNCTION ----------\ndef train_predict_evaluate(\n    df: pd.DataFrame,\n    target_col: str,\n    selected_model: str,\n    forecast_years: float,\n    # Kept arguments for compatibility but they are less relevant in direct mode\n    seq_length: int = 1, \n    energy_lags: int = 0,\n    keep_predictor_names = None,\n    xgb_params: dict = None\n):\n    \"\"\"\n    Train selected_model(s) and forecast using Direct Regression Strategy.\n    \n    Logic matches the XGBoost style:\n    1. Train on [Features + Time] -> [Target]\n    2. Create Future Data using Seasonal Estimation\n    3. Predict entirely at once (No recursive loops)\n    \"\"\"\n    \n    # 1. Setup Defaults\n    if xgb_params is None:\n        xgb_params = dict(\n            n_estimators=100, learning_rate=0.03, max_depth=6,\n            subsample=0.9, colsample_bytree=0.9,\n            objective='reg:squarederror', n_jobs=-1, random_state=42\n        )\n\n    if target_col not in df.columns:\n        return None, None, None, None, f\"Target {target_col} not found!\"\n        \n    # Identify feature columns (everything numeric except target)\n    feature_cols = [c for c in df.select_dtypes(include=np.number).columns if c != target_col]\n    \n    # 2. Prepare Data (XGBoost Style)\n    X, y, x_col_names, df_processed = prepare_features_direct(df, target_col, feature_cols)\n    \n    # Scale X for Neural Nets compatibility (XGB handles unscaled fine, but scaling hurts nothing)\n    scaler_x = MinMaxScaler()\n    X_scaled = scaler_x.fit_transform(X)\n    \n    # Scale Y usually helps DL convergence\n    scaler_y = MinMaxScaler()\n    y_scaled = scaler_y.fit_transform(y.reshape(-1, 1)).flatten()\n    \n    # Split\n    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y_scaled, test_size=0.2, shuffle=False)\n    \n    # 3. Train Models\n    models = {}\n    history = {}\n    \n    if selected_model in ['Ensemble', 'Weighted_Avg']:\n        types_to_train = ['LSTM', 'GRU', 'XGBoost']\n    else:\n        types_to_train = [selected_model]\n        \n    input_dim = X_train.shape[1]\n    \n    for m_type in types_to_train:\n        print(f\"Training {m_type}...\")\n        \n        if m_type == 'XGBoost':\n            model = xgb.XGBRegressor(**xgb_params)\n            model.fit(X_train, y_train)\n            models[m_type] = model\n            history[m_type] = {} # XGB doesn't return keras-style history objects\n            \n        elif m_type in ('LSTM', 'GRU'):\n            # Reshape X for RNN: (Samples, Timesteps=1, Features)\n            X_train_dl = X_train.reshape((X_train.shape[0], 1, X_train.shape[1]))\n            X_test_dl = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n            \n            model = build_dl_regressor(m_type, input_dim)\n            es = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True, verbose=0)\n            \n            hist = model.fit(\n                X_train_dl, y_train,\n                validation_data=(X_test_dl, y_test),\n                epochs=50, \n                batch_size=32, \n                verbose=0,\n                callbacks=[es]\n            )\n            models[m_type] = model\n            history[m_type] = hist.history\n\n    # 4. Calculation Weights (if Weighted_Avg)\n    weights = {}\n    if selected_model == 'Weighted_Avg':\n        errors = {}\n        for name, m in models.items():\n            if name == 'XGBoost':\n                p = m.predict(X_test)\n            else:\n                X_test_dl = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n                p = m.predict(X_test_dl, verbose=0).flatten()\n            errors[name] = np.sqrt(mean_squared_error(y_test, p))\n        \n        # Inverse error weighting\n        inv = {k: 1.0 / (v + 1e-6) for k, v in errors.items()}\n        total = sum(inv.values())\n        weights = {k: v / total for k, v in inv.items()}\n\n    # 5. Forecasting (XGBoost Style: Direct Prediction on Future Features)\n    X_future, future_dates = create_future_dataframe(df_processed, feature_cols, forecast_years)\n    X_future_scaled = scaler_x.transform(X_future)\n    \n    # Collect predictions from all models\n    future_preds_collection = []\n    \n    # Order models consistent with training\n    model_names = list(models.keys())\n    \n    for name in model_names:\n        m = models[name]\n        if name == 'XGBoost':\n            pred = m.predict(X_future_scaled)\n        else:\n            X_future_dl = X_future_scaled.reshape((X_future_scaled.shape[0], 1, X_future_scaled.shape[1]))\n            pred = m.predict(X_future_dl, verbose=0).flatten()\n        future_preds_collection.append(pred)\n    \n    # Combine Predictions\n    if selected_model == 'Weighted_Avg':\n        final_scaled_pred = np.zeros_like(future_preds_collection[0])\n        for i, name in enumerate(model_names):\n            final_scaled_pred += future_preds_collection[i] * weights[name]\n            \n    elif selected_model == 'Ensemble':\n        final_scaled_pred = np.mean(future_preds_collection, axis=0)\n        \n    else:\n        # Single model case\n        final_scaled_pred = future_preds_collection[0]\n        \n    # Inverse Transform Target (Scale back to original units)\n    final_forecast = scaler_y.inverse_transform(final_scaled_pred.reshape(-1, 1)).flatten()\n    \n    # 6. Metrics Calculation (on Test Set)\n    test_preds_collection = []\n    for name in model_names:\n        m = models[name]\n        if name == 'XGBoost':\n            p = m.predict(X_test)\n        else:\n            X_test_dl = X_test.reshape((X_test.shape[0], 1, X_test.shape[1]))\n            p = m.predict(X_test_dl, verbose=0).flatten()\n        test_preds_collection.append(p)\n        \n    if selected_model == 'Weighted_Avg':\n        final_test_pred_scaled = np.zeros_like(test_preds_collection[0])\n        for i, name in enumerate(model_names):\n            final_test_pred_scaled += test_preds_collection[i] * weights[name]\n    elif selected_model == 'Ensemble':\n        final_test_pred_scaled = np.mean(test_preds_collection, axis=0)\n    else:\n        final_test_pred_scaled = test_preds_collection[0]\n        \n    # Inverse Test Metrics\n    inv_y_test = scaler_y.inverse_transform(y_test.reshape(-1, 1)).flatten()\n    inv_pred_test = scaler_y.inverse_transform(final_test_pred_scaled.reshape(-1, 1)).flatten()\n    \n    mse = mean_squared_error(inv_y_test, inv_pred_test)\n    mae = mean_absolute_error(inv_y_test, inv_pred_test)\n    r2 = r2_score(inv_y_test, inv_pred_test)\n    \n    metrics_str = f\"{selected_model} Results:\\nMSE: {mse:.2f}\\nMAE: {mae:.2f}\\nR2: {r2:.4f}\"\n    \n    if selected_model == 'Weighted_Avg':\n        w_str = \"\\n\".join([f\"{k}: {v:.3f}\" for k, v in weights.items()])\n        metrics_str += f\"\\n\\nWeights:\\n{w_str}\"\n\n    return final_forecast, history, metrics_str, inv_y_test, inv_pred_test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T07:53:30.295682Z","iopub.execute_input":"2026-01-27T07:53:30.296347Z","iopub.status.idle":"2026-01-27T07:53:30.323081Z","shell.execute_reply.started":"2026-01-27T07:53:30.296319Z","shell.execute_reply":"2026-01-27T07:53:30.322408Z"}},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":"# # 6. Forecasting Dashboard (Deployment)\n A widget-based control panel allows users to select a plant, target variable, and model to generate future forecasts. It displays the forecast plot, training loss history, and evaluation metrics, and provides a CSV download of the results","metadata":{}},{"cell_type":"code","source":"# %% [code]\nimport ipywidgets as widgets\nfrom IPython.display import display, FileLink\nimport plotly.graph_objects as go\nimport pandas as pd\nimport datetime\n\n# Ensure plants are loaded from data_store\nplants = list(data_store.keys())\n\n# --- Widgets ---\nw_plant = widgets.Dropdown(\n    options=plants,\n    value=plants[0] if plants else None,\n    description='Plant:',\n    style={'description_width': 'initial'}\n)\n\nw_target = widgets.Dropdown(\n    description='Target:',\n    style={'description_width': 'initial'}\n)\n\nw_model = widgets.Dropdown(\n    options=['LSTM', 'GRU', 'XGBoost', 'Ensemble', 'Weighted_Avg'],\n    value='Weighted_Avg',\n    description='Model:',\n    style={'description_width': 'initial'}\n)\n\nw_years = widgets.IntSlider(\n    min=1,\n    max=10,\n    step=1,\n    value=4,\n    description='Years:',\n    style={'description_width': 'initial'}\n)\n\nw_run = widgets.Button(\n    description='Run Forecast',\n    button_style='success',\n    icon='play'\n)\n\noutput_area = widgets.Output()\n\n# --- Logic: Update Targets to Only Energy & Water_Level ---\ndef update_targets(change=None):\n    if not w_plant.value:\n        return\n    df = data_store[w_plant.value]\n    \n    options = []\n    # Explicitly check and add ONLY these two columns\n    desired_targets = ['Energy', 'Water_Level']\n    \n    for t in desired_targets:\n        if t in df.columns:\n            options.append(t)\n            \n    # Note: We removed the loop that added other numeric columns\n    # so the dropdown will ONLY show Energy or Water_Level\n    \n    w_target.options = options\n    w_target.value = options[0] if options else None\n\nw_plant.observe(update_targets, names='value')\nupdate_targets() # Initialize\n\n# --- Run Logic ---\n# Updated run_forecast (notebook cell) ‚Äî show up to 7 years of historical data for clearer seasonality\ndef run_forecast(b):\n    output_area.clear_output()\n    with output_area:\n        plant = w_plant.value\n        target = w_target.value\n        model_type = w_model.value\n        years = w_years.value\n\n        if not plant or not target:\n            print(\"Please select all options.\")\n            return\n\n        print(f\"üîÑ Running {model_type} forecast for {plant} - Target: {target} ({years} years)...\")\n        \n        try:\n            # Calls the train_predict_evaluate function from the previous cell\n            forecast, history, metrics_txt, val_actual, val_pred = train_predict_evaluate(\n                data_store[plant], target, model_type, years\n            )\n        except Exception as e:\n            print(f\"‚ùå Error: {e}\")\n            return\n\n        if forecast is None:\n            print(\"‚ùå Forecast failed.\")\n            return\n\n        # Ensure the index is datetime\n        df_plant = data_store[plant].copy()\n        if not pd.api.types.is_datetime64_any_dtype(df_plant.index):\n            try:\n                df_plant.index = pd.to_datetime(df_plant.index)\n            except Exception:\n                print(\"‚ùå Error: plant dataframe index is not datetime and could not be converted.\")\n                return\n\n        # Dates Calculation for forecast\n        last_date = df_plant.index[-1]\n        future_dates = [last_date + datetime.timedelta(days=i) for i in range(1, len(forecast) + 1)]\n\n        # HISTORICAL RANGE: up to 7 years (adjust if less data available)\n        max_hist_days = 7 * 365\n        available_days = len(df_plant)\n        hist_days = min(available_days, max_hist_days)\n        hist_series = df_plant[target].iloc[-hist_days:].copy()\n\n        # 1. Forecast Plot (7-year historical)\n        fig_forecast = go.Figure()\n        fig_forecast.add_trace(\n            go.Scatter(\n                x=hist_series.index,\n                y=hist_series.values,\n                mode='lines',\n                name=f'Historical (last {hist_days} days ‚âà {hist_days//365} years)',\n                line=dict(color='blue', width=2),\n                opacity=0.8\n            )\n        )\n        fig_forecast.add_trace(\n            go.Scatter(\n                x=future_dates,\n                y=forecast,\n                mode='lines+markers',\n                name='Forecast',\n                line=dict(color='red', width=2),\n                marker=dict(size=4)\n            )\n        )\n        # Improve readability: vertical line separating history and forecast\n        fig_forecast.add_vline(x=last_date, line=dict(color='gray', dash='dash'))\n        fig_forecast.update_layout(\n            title=f\"{model_type} Forecast: {target} ({years} Years) ‚Äî Historical shown up to 7 years\",\n            xaxis_title=\"Date\",\n            yaxis_title=target,\n            height=500,\n            legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1.02, xanchor=\"right\", x=1)\n        )\n        fig_forecast.show()\n\n        # 2. Training Loss Plot (defensive plotting)\n        fig_loss = go.Figure()\n        for m_name, h in (history or {}).items():\n            if not isinstance(h, dict):\n                continue\n            train_loss = h.get('loss') or h.get('training_loss') or []\n            val_loss = h.get('val_loss') or h.get('validation_loss') or []\n            if train_loss:\n                fig_loss.add_trace(go.Scatter(\n                    x=list(range(1, len(train_loss) + 1)),\n                    y=train_loss,\n                    mode='lines',\n                    name=f'{m_name} Train Loss'\n                ))\n            if val_loss:\n                fig_loss.add_trace(go.Scatter(\n                    x=list(range(1, len(val_loss) + 1)),\n                    y=val_loss,\n                    mode='lines',\n                    name=f'{m_name} Val Loss',\n                    line=dict(dash='dash')\n                ))\n        if not fig_loss.data:\n            print(\"\\nNo training history available to plot (no 'loss' or 'val_loss' keys).\")\n        else:\n            fig_loss.update_layout(\n                title=\"Training Loss\",\n                xaxis_title=\"Epoch\",\n                yaxis_title=\"Loss\",\n                height=350\n            )\n            fig_loss.show()\n\n        # 3. Metrics\n        print(\"\\n‚úÖ Evaluation Metrics:\")\n        print(metrics_txt)\n        \n        # 4. CSV Download\n        df_out = pd.DataFrame({\n            'Date': future_dates,\n            f'Forecast_{target}': forecast\n        })\n        filename = f\"{plant}_{target}_Forecast_{years}Y.csv\".replace(\" \", \"_\")\n        df_out.to_csv(filename, index=False)\n        print(\"\\n‚¨áÔ∏è Download Forecast:\")\n        display(FileLink(filename))\n\nw_run.on_click(run_forecast)\n\n# --- Layout ---\nui = widgets.VBox([\n    widgets.HTML(\"<h2 style='text-align:center;'>EEP Hydropower Forecasting Dashboard (Kaggle)</h2>\"),\n    widgets.HBox([w_plant, w_target]),\n    widgets.HBox([w_model, w_years]),\n    w_run,\n    output_area\n])\n\ndisplay(ui)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-01-27T08:15:53.878510Z","iopub.execute_input":"2026-01-27T08:15:53.878823Z","iopub.status.idle":"2026-01-27T08:15:53.916455Z","shell.execute_reply.started":"2026-01-27T08:15:53.878799Z","shell.execute_reply":"2026-01-27T08:15:53.915650Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value=\"<h2 style='text-align:center;'>EEP Hydropower Forecasting Dashboard (Kaggle)</h2>\")‚Ä¶","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd0a9f26fad846dca24218f1714de4e2"}},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}